{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No image found on page or error: https://openlittermap.com/global?lat=32.757176519402&lon=-117.14407081727&zoom=17&photo=309226\n",
      "No image found on page or error: https://openlittermap.com/global?lat=32.757182302915&lon=-117.14453215722&zoom=17&photo=309760\n",
      "Found image URL: https://olm-s3.s3.eu-west-1.amazonaws.com/2021/04/25/IMG_3893.HEIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/mrcq87v97y184l_h_jmgspdr0000gn/T/ipykernel_95739/3398765617.py:89: DtypeWarning: Columns (289,290,291) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  final_data = pd.read_csv(checkpoint_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './data/combined_tabular_data.csv'\n",
    "checkpoint_path = './data/checkpoint_data.csv'\n",
    "\n",
    "write_lock = threading.Lock()\n",
    "\n",
    "# Initialize or load the checkpoint file\n",
    "if os.path.exists(checkpoint_path):\n",
    "    file = pd.read_csv(checkpoint_path, low_memory=False)\n",
    "else:\n",
    "    file = pd.read_csv(file_path, low_memory=False)\n",
    "    file['image_url'] = None\n",
    "    file.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "def scrape_page(data_chunk):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless=new')\n",
    "\n",
    "    updates_buffer = []\n",
    "    update_count = 0\n",
    "\n",
    "    for index, row in data_chunk.iterrows():\n",
    "\n",
    "        if pd.notna(row['image_url']):\n",
    "            continue\n",
    "\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "        page_url = row['link']\n",
    "        driver.get(page_url)\n",
    "\n",
    "        try:\n",
    "            image_tag = WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'img.leaflet-litter-img'))\n",
    "            )\n",
    "            image_url = image_tag.get_attribute('src')\n",
    "            updates_buffer.append((row['id'], image_url))\n",
    "            \n",
    "            print(f'Found image URL: {image_url}')\n",
    "\n",
    "            update_count += 1\n",
    "            if update_count >= 15:\n",
    "                with write_lock:\n",
    "                    temp_file = pd.read_csv(checkpoint_path, low_memory=False)\n",
    "                    print(temp_file.image_url.isna().sum())\n",
    "                    for id, url in updates_buffer:\n",
    "                        temp_file.loc[temp_file['id'] == id, 'image_url'] = url\n",
    "                    temp_file.to_csv(checkpoint_path, index=False)\n",
    "                    updates_buffer = []\n",
    "                    update_count = 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'No image found on page or error: {page_url}')\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "    # Ensure any remaining updates are written out after loop completion\n",
    "    if updates_buffer:\n",
    "        with write_lock:\n",
    "            temp_file = pd.read_csv(checkpoint_path, low_memory=False)\n",
    "            for id, url in updates_buffer:\n",
    "                temp_file.loc[temp_file['id'] == id, 'image_url'] = url\n",
    "            temp_file.to_csv(checkpoint_path, index=False)\n",
    "\n",
    "# # Split DataFrame into chunks\n",
    "# num_chunks = 4\n",
    "# chunks = np.array_split(file, num_chunks)\n",
    "\n",
    "# # Process chunks in parallel\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     executor.map(scrape_page, chunks)\n",
    "\n",
    "scrape_page(file)\n",
    "\n",
    "# Save the final DataFrame to a file\n",
    "final_data = pd.read_csv(checkpoint_path)\n",
    "final_data.to_excel('./data/final_output_with_images.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af46d77f25f6268fb52e7aa039aee65944f4abeb7c36e07a63c4427d9c88eec5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
